---
title: "bdacars"
output:
  html_document:
    df_print: paged
---

## Introduction

If you're reading this, you have probably heard about mtcars. If not, you're about to.

Originally extracted from the US magazine Motor Trend in the 1970s, this dataset of car performance and design information has inspired a load of applications of statistical models, notably in regression. For some reason, we have never actually analyzed this dataset on any course before, and now is our chance to take a swing at it, the Bayesian way. Here is a quick summary of what is about to go down:

- Overview of the dataset and the analysis problem
- Exploratory analysis
- Modeling: prior distributions, structure of the models
- Model diagnostics and results
- Conclusion & Discussion

Avoiding any more spoilers, let's see what we have on our hands:

### The dataset

The dataset consists of 32 cars, for which the following information has been recorded:

- Miles per gallon  (a measure of fuel consumption)
- Number of cylinders
- Displacement (the total volume of the cylinders)
- Gross horsepower
- Rear axle ratio (related to towing capabilities)
- Weight
- Quarter mile time (how fast the car can traverse a quarter mile)
- Shape of the engine - straight vs V-shaped
- Transmission  - automatic vs manual
- Number of forward gears
- Number of carburetors

The dataset naturally calls for a regression model to study the effect of car design choices on fuel consumption. Here's our approach:

### The analysis problem

We will analyze three different factors potentially affecting fuel consumption: car weight, displacement and transmission. We hypothesize that heavier cars with bigger engines should consume more fuel. Although, we do assume that weight and displacement are correlated, so it'll be interesting to see how we can separate the two effects.  We also hypothesize that automatic cars consume less fuel than manual cars, since the automatic transmission can optimize gears used better than a human driver.

To find out which combination of the three variables best predict fuel consumption, we will try four different models: one model for each pair of predictors and a model with all three included. We will then choose the best model and continue with the analysis.

### Unit conversions

The dataset is quite clean: no missing data or any nasty things to deal with. There is one problem we need to tackle right away, though. The units. Miles per gallon is not very easy to interpret to a Finnish person, especially because we are used to measuring fuel consumption directly, instead of measuring how far we can get with a fixed volume of fuel. Also, car displacement is recorded in cubic inches and car weight in pounds. The first step is to convert them to Finnish units so we don't need to do mental work interpreting every single figure and coefficient. Here are the conversions:

- Fuel consumption: Miles per US gallon $\rightarrow$ litres per 100 km
- Weight: 1000 pounds $\rightarrow$ metric tons (1000 kg)
- Displacement: Cubic inches $\rightarrow$ litres

Before deciding on further preprocessing steps, we need to explore the data a bit.

## Exploratory analysis

Setting up libraries and the project path:

```{r, message=FALSE}
library(rstan)
library(GGally)
library(dplyr)
library(loo)
library(gridExtra)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
path <- "C:/Users/esatu/Dropbox/Aalto/Bayesian Data Analysis/Project/"
#path <- "~/Google_Drive/Kurssit/BDA/project/bdacars/"
source(paste0(path, "functions.R"))
```

Apply the unit conversions:

```{r}
propercars <- data.frame(wt = mtcars$wt/2.205,
                         disp = mtcars$disp/61.024,
                         lp100km = 235.215/mtcars$mpg,
                         am = mtcars$am)
```

For clarity, the abbreviations for the variables we use are

- wt = weight
- disp = displacement
- lp100km = litres per 100 km
- am = type of transmission, 0 = automatic, 1 = manual

Now that we got the units right, let's take a lok at the data! A good first step is to plot the variables against each other.

```{r, fig.width=10, fig.height=10}
# Change am to factor for correct plotting
plotcars <- propercars
plotcars$am <- factor(propercars$am)
# Change default plots styled for the lower triangular of the plot matrix
lower <- list(continuous = "smooth",
                combo = "facetdensity")
# Plot pairs of variables against each other
ggpairs(plotcars,
        lower = lower,
        showStrips = TRUE,
        progress = FALSE,
        title = "Original data in Finnish units") +
  theme_bw()
```

The plot matrix is a lot to take in, so let's deal with it piece-by-piece.  
First, let's look at the third row. It displays fuel consumption as a function of the potential explanatory variables. Starting from the left, the first three plots show no surprises. Fuel consumption strongly correlates with car weight and displacement, and the distribution looks smooth with few cars deviating from the rest. The last plot, though, seems to contradict one of our initial hypotheses as manual cars (class 1) seem to have lower fuel consumption than automatic ones. The bottom row displays a closer view at the distribution of the consumption in the two transmission classes. The distribution of automatic cars seems to follow the overall distribution, while cars with low fuel consumption dominate the manual class.  We'll have to come back to this later!

Next up, the top two rows representing the relationships between our potential explanatory variables. The short story is: they're correlated. The Pearson correlation coefficient between weight and displacement is just shy of 0.9. In addition, automatic cars seem to be heavier and equipped with larger engines than manual cars. Maybe this explains their higher fuel consumption! We'll know more after we run our models.

### Preprocessing

All the relationships in the data seem to be linear, so there is no clear need for non-linear transformation of variables. We will perform standard procedures of mean-centering and scaling by standard deviation before modeling. While mean centering does not change the regression coefficients, it makes the interpretation of the model intercepts more meaningful. Scaling by standard deviation in addition to mean centering makes our variables have mean 0 and standard deviation of 1. This will make choosing priors for the regression coefficients easier, as we know we are dealing with unit scale variables.

```{r}
# Only scale numeric variables
scaledcars <- propercars
scaledcars[1:3] <- scale(scaledcars[1:3])
```

We are now ready to move to the modeling part!

## Modeling

### Linear models

We perform Bayesian linear regression, i.e. we model fuel consumption using a linear models of form:
$$y \sim N(\alpha + \beta X,\, \sigma^2)$$
where $y$ is fuel consumption, $X$ is the matrix of predictor variables and $\alpha, \beta$ are the intercept and regression coefficients, respectively.

As mentioned before, we will try out four different models: one model for each pair of predictors and a model with all three included. We will then choose the best model and continue with the analysis.

### Priors

We use weakly informative priors, recommended as good default priors in [Stan prior choice recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations). The prior for $\alpha$ is a Cauchy distribution with center 0 and scale 10. As priors for the regression coefficients $\beta$ we use a Student's t distribution with 3 degrees of freedom, location 0 and scale 2. For the standard deviation of the posterior distribution, $\sigma$ we use a half-normal (0, 10) distribution. More information about the prior distributions and why they are weakly informative can be found in the appendix.

### Stan model

The Stan code for the model with all the three predictors included looks like this:

```{stan, output.var="esa", eval=FALSE}
data {
  int<lower=0> n;
  vector[n] wt;
  vector[n] disp;
  vector[n] am;
  vector[n] lp100km;
}

// We name the components of beta explicitly
// While a bit cumbersome, this helps interpretation
// and lowers chances of error
parameters {
  real alpha;
  real beta_wt;
  real beta_disp;
  real beta_am;
  real<lower=0> sigma;
}
// The conditional mean for the linear model
transformed parameters{
  vector[n] mu;
  mu = alpha + beta_wt*wt + beta_disp*disp + beta_am*am;
}

model {
  // Priors
  alpha ~ cauchy(0,10);
  beta_wt ~ student_t(3,0,2);
  beta_disp ~ student_t(3,0,2);
  beta_am ~ student_t(3,0,2);
  sigma ~ normal(0, 10);
  // The linear model
  lp100km ~ normal(mu, sigma);
}

// Log likelihoods genereated for LOO
generated quantities {
  vector[n] log_lik;
  for (i in 1:n)
    log_lik[i] = normal_lpdf(lp100km[i] |mu[i] , sigma);
}
```

The other models are identical, with one of the predictor variables removed.

It is about time we see some results. Let's set up the data into a Stan-friendly format:

```{r}
dada <- list(lp100km = scaledcars$lp100km,
             wt=scaledcars$wt,
             disp = scaledcars$disp,
             am = scaledcars$am,
             drat = scaledcars$drat,
             n = nrow(scaledcars))
```

And run the Stan models:

```{r, echo = T, results = 'hide'}
model_names <- c("wt_disp", "wt_am", "disp_am", "wt_disp_am")
models <- list()

for(mname in model_names){
  fit <- stan(paste0(path, "stan_models/", mname,".stan"), data=dada, model_name = mname, seed = 38)
  models[[mname]] <- fit
}
```

Before comparing the performance of the models, we need to ensure a fair contest by checking that all of them have converged. Here are the basic model summaries:

```{r, results='asis'}
for(mname in model_names){
  # Extract convergence diagnostics from the model
  smry <- model_summary(models[[mname]], mname)
  # Output as a nice table
  print(knitr::kable(smry, caption = paste("lp100km ~", gsub("_", " + ", mname)), digits = 3))
}
```

Since all the $\hat{R}$ values are close to 1, we can conclude that all the models have converged. Next, we will take a look at divergences to see if our MCMC estimators have explored the target distributions adequately.

```{r}
for(mname in model_names){
  check_divergences(models[[mname]])
}
```
None of the models exhibit any divergences. This is to be expected as our models were simple linear models, which are not particularly prone to pathologic target distributions. 

We can now compare the models to find the best model for further analysis.

### Model comparison

We will use leave-one-out cross validation (LOO-cv) to compare the performance of the models:

```{r}
# Initialize
psisloos <- c()
p_effs <- c()
k_plots <- list()
for(mname in model_names){
  # Run LOO-cv
  loglik <- extract_log_lik(models[[mname]], merge_chains = FALSE)
  r_eff <- relative_eff(exp(loglik))
  loocv <- suppressWarnings(loo(loglik, r_eff = r_eff))
  
  # PSIS-LOO value
  psisloo <- loocv$estimates[1]
  # Effective parameters
  p_eff <- loocv$estimates[2]
  # k-values
  kval <- loocv$diagnostics$pareto_k
  
  # Combine results from the models
  p_effs <- c(p_effs, p_eff)
  psisloos <- c(psisloos, psisloo)
  
  # Plot of k-values
  kvals <- data.frame(k = kval,
                      x = 1:length(kval))
  p <- ggplot(data=kvals, aes(x=x, y=k)) +
    geom_point(color = 'red') +
    geom_hline(yintercept=0.5, color = 'blue') +
    ggtitle(mname) +
    xlab("") +
    ylab("k-value") +
    theme_bw()
  k_plots <- c(k_plots, list(p))
}
```

The first step is to see if the k-values of LOO-cv are low enough so that we can trust the results:

```{r, fig.width=10, fig.height=10}
# Combine plots of the k-values
grid.arrange(k_plots[[1]], k_plots[[2]], k_plots[[3]], k_plots[[4]], nrow=4, ncol=1)
```

With a few exceptions, most of the k-values seem to be low enough so that we can trust the results. The only one of the models that is raising some concern is the one with all the explanatory variables.

Next, let's look at the PSIS-LOO values. The higher the value, the better the performance of the model. Winner takes it all, other models are discarded before we continue any further.


```{r, collapse=TRUE, fig.align="center"}
psisloosdf <- data.frame(psisloo = psisloos,
                          model = model_names)
ggplot(data=psisloosdf, aes(x=model, y=psisloo)) + geom_bar(stat="identity", width=0.5) +
   geom_hline(yintercept=max(psisloos), color = 'blue') +
   theme_bw()

knitr::kable(psisloosdf %>%  arrange(desc(psisloo)),
             caption = "PSIS-LOO values", digits = 3)
```

The best model is the one with only weight and displacement as predictors, and adding transmission type to the model does not improve performance. So it seems that the difference in fuel consumption between automatic and manual cars was indeed primarily due to the fact that the automatic cars were heavier and equipped with larger engines. Or at least we would need a more diverse dataset to make inferences about the effect of the transmission.

### Model diagnostics

Let's confirm the explanatory power of our selected predictors by examining the heteroskedasticity of our model visually. We calculate residuals by using the means of our parameter distributions to procure predicted values.

```{r, fig.width=8, fig.align="center"}
alpha <- extract(models[["wt_disp"]], "alpha")
beta_wt <- extract(models[["wt_disp"]], "beta_wt")
beta_disp <- extract(models[["wt_disp"]], "beta_disp")	

mean_alpha <- mean(alpha$alpha)
mean_beta_wt <- mean(beta_wt$beta_wt)
mean_beta_disp <- mean(beta_disp$beta_disp)

# Residuals
predicted <- mean_alpha + mean_beta_wt*scaledcars$wt + mean_beta_disp*scaledcars$disp
residuals <- scaledcars$lp100km - predicted

resids_df <- data.frame(scaledcars$wt, scaledcars$disp, residuals)
colnames(resids_df) <- c("Weight", "Displacement", "Residuals")

p <- ggplot(resids_df, aes(resids_df$Weight, resids_df$Residuals)) +
  geom_point(color='red') +
  geom_hline(yintercept = 0) +	
  scale_y_continuous(name="Residuals") +	
  scale_x_continuous(name="Weight")	+
  theme_bw()
p
```
The resulting plot exhibits significant heteroskedasticity. This could be an indicator of poorly chosen parameters, i.e. fuel consumption is affected by information we haven't included in our model. Let's take a quick look at the original dataset and its correlation matrix.

```{r, fig.width=8, fig.height=8, fig.align="center"}
library(lattice)
mat <- cor(mtcars)
levelplot(mat)
```
The correlation matrix above would indicate that in addition to engine displacement and car weight, the number of cylinders, gross horsepower and number of carburetors are predictors that might benefit our models. Nevertheless, we've already chosen the scope of our study and will continue with the best model within our predictor space. We'll begin with posterior predictive checking to see if our model produces believable measurements.

```{r, fig.width = 8, fig.align="center"}
exmu <- extract(models[["wt_disp"]], "mu")$mu
exsigma <- extract(models[["wt_disp"]], "sigma")$sigma

# Sample from the posterior predictive distribution
set.seed(38)
samples <- matrix(0, nrow=nrow(exmu), ncol=ncol(exmu))
for(i in 1:nrow(exmu)){
  for(j in 1:ncol(exmu)){
    samples[i,j] <- rnorm(1, mean=exmu[i,j], sd = exsigma[i])
  }
}

# Plot densities of the original and the predictive distribution
plot_df <- data.frame(lp100km = c(as.vector(samples), scaledcars$lp100km),
                      Distribution = rep(c("Posterior", "Original"), times=c(length(samples), nrow(scaledcars))))
ggplot(plot_df, aes(lp100km, color = Distribution)) +
  geom_density() +
  scale_color_brewer(palette = "Set1") +
  theme_bw()

```

The posterior predictive distribution is very close to the original one, although it is less favorable to the high fuel consumption. The predictive distribution seems credible.

## Results

Let us next inspect our parameter space more closely.
```{r}
fit <- models[["wt_disp"]]
```

Since we only have two regression coefficients, it is easy for us to visualize the parameter space:

```{r, fig.align="center"}
fit_df <- as.data.frame(fit)[1:4]

p_params <- ggplot(fit_df, aes(beta_wt, beta_disp, color=sigma)) +
  geom_point(alpha = 0.3) +
  geom_density2d(color = "white") +
  scale_color_viridis_c(option = 'inferno') +
  labs(title = 'Stan parameter space') +
  theme_bw()
p_params
```

There is a negative correlation between the two parameters, which makes sense, since if one of the predictors has a high coefficient, there is not much variation of fuel consumption left for the other predictor to explain (remember that the predictors weight and displacement are themselves heavily correlated).

Let's look at the individual distributions of the parameters:

```{r, fig.width=10, fig.height=4}
wt_p <- ggplot(fit_df, aes(beta_wt)) +
  geom_histogram(bins = 50, fill = "grey70", color = "grey50") +
  theme_bw()

disp_p <- ggplot(fit_df, aes(beta_disp)) +
  geom_histogram(bins = 50, fill = "grey70", color = "grey50") +
  theme_bw()

grid.arrange(wt_p, disp_p, ncol = 2)
```

For both of the regression coefficients, the vast majority of the draws are positive. To get a numeric estimate of the relevance of the two parameters, let's take a closer look at the summary statistics of the model:

```{r}
beta_smry <- summary(fit, pars = c("beta_wt", "beta_disp"))$summary
knitr::kable(beta_smry, digits = 3)
```

We also want to know the probability that heavier cars and larger engines increase the fuel consumption:

```{r}
fit_df %>% select(beta_wt, beta_disp) %>% sapply(function(x){sum(x > 0)/length(x)})
```

Unsurprisingly, we can conclude that  both weight and displacement are related to fuel consumption with a very high probability. But what about the effect size? We have so far only reported the effect size in the scaled units. It is not sufficient to know that heavier cars consume more fuel, we want to know _how much more_. Let's take some key statistics of the last table and transform them back to human readable real world units.

```{r}
# Compute standard deviations of the predictors
sds <- sapply(propercars[c("wt", "disp", "lp100km")], sd)
# Scale back to original units
orig_smry <- beta_smry[, c(1, 4:8)]
orig_smry[1,] <- orig_smry[1,]/sds["wt"] * sds["lp100km"]
orig_smry[2,] <- orig_smry[2,]/sds["disp"] * sds["lp100km"]
knitr::kable(orig_smry, digits = 3)
```

Now this is easier to interpret! So, an increase of a ton in car weight will on average cause an increase of approximately 4.5 liters per 100 km in fuel consumption. Meanwhile, increasing engine displacement by one litre results in an increase of approximately 0.8 litres per 100km in fuel consumption.

## Sensitivity analysis

We believe the priors are well chosen, but let's check how much the results would change if we used a different set of priors, let's say the default noninformative priors of Stan. The model and the code are similar to above, and can be viewed in the appendix. Let's take a look at the parameter space first:

```{r, echo=FALSE, fig.width=12, fig.height=6}
fit_noninformative <- stan(paste0(path, "stan_models/wt_disp_noninformative.stan"), data=dada, model_name = "noninf", seed = 38)

fit_df$prior <- "Original"
fit_df2 <- as.data.frame(fit_noninformative)[1:4]
fit_df2$prior <- "Noninformative"

fit_df_both <- rbind(fit_df, fit_df2)
fit_df_both$prior <- factor(fit_df_both$prior, levels = c("Original", "Noninformative"))

p_params2 <- ggplot(fit_df_both, aes(beta_wt, beta_disp, color=sigma)) +
  geom_point(alpha = 0.3) +
  geom_density2d(color = "white") +
  scale_color_viridis_c(option = 'inferno') +
  labs(title = 'Stan parameter space with different priors') +
  theme_bw() +
  facet_grid(cols=vars(prior))
p_params2
```

The joint distribution of the parameters looks quite similar even with a quite large change in the prior distribution.

```{r, echo=FALSE, fig.width=10, fig.height=4}
wt_p2 <- ggplot(fit_df_both, aes(beta_wt, color=prior)) +
  geom_density() +
  theme_bw()

disp_p2 <- ggplot(fit_df_both, aes(beta_disp, color = prior)) +
  geom_density() +
  theme_bw()

grid.arrange(wt_p2, disp_p2, ncol = 2)
```

... and so do the marginal distributions. We conclude that the posterior distribution is mainly shaped by the data, and the effect of the priors is quite insignificant.

## Conclusion and discussion

We were interested in the effects that car weight, engine displacement and transmission type has on fuel consumption with regards to the Motor Trends dataset from the 1970s. Our initial exploratory analysis revealed that especially weight and engine displacement would be great predictors for further modeling. Additionally, the data on transmission type subverted our expectations by showing less fuel consumption for manual transmission. There were hints in the data that the result might simply be caused by imbalances regarding weight and displacement distributions. We built four linear models with weakly informative priors to properly stack the predictors against each other, and found out that indeed, the model with weight and displacement performed best, assessing by PSIS-LOO. Including the transmission type into the model had more of a noising effect, which lends credit to our initial idea of weight/displacement imbalances in the transmission type categories.

We end up concluding that both weight and engine displacement have a very high probability of increasing fuel consumption, with an effect size 4.5 l/100km per a tonne of added weight and 0.8 l/100km per a litre of increased displacement. 

Our model is not perfect however, as it exhibits significant heteroskedasticity when examining our residuals. There are two kinds of heteroskedasticity: impure, which arises from excluding relevant information from the model, and pure, which arises from using predictors with non-constant variance. It might be that we've ignored a great sum of relevant information by only focusing on 3 predictors, or it might be that we've happened to either choose or manipulate our predictors in such a way that instroduces large non-constant variance into our model data. Our study could be improved by trying to pinpoint the nature of the heteroskedasticity either by expanding the range of predictors included in the model, or by inspecting the variances of the predictors before and after manipulation. Nevertheless, our model produces a credible posterior predictive distribution and is deemed usable as such.

## Appendix

### Prior distributions

In the report, we didn't explain how we ended up with the scale 2 for the t-distributions and why the priors can indeed be regarded as weakly informative.

The rule of thumb for weakly informative distributions is that the standard deviation of the posterior distribution should be less than 0.1 times that of the prior. The scales of the priors were chosen so that this rule is respected. To do this, we need to calculate the standard deviation of the prior. For a t-distribution with scale $s$ and $\nu$ degrees of freedom, the standard deviation is:
$$\sqrt{s^2\cdot \frac{\nu}{\nu -2}} = \sqrt{2^2 * \frac{3}{3-2}} = 2\cdot\sqrt{3} \approx 3.46$$
For a Cauchy distribution, the standard deviation is not defined, and for the normal prior of $\sigma$ we use standard deviation of 10.

Looking at the model summaries shown in the main report, we can see that the the standard deviations of all the parameters are below 0.1 times the standard deviations of the priors, as we wanted.

### Sensitivity analysis

Code for fitting the model and visualizing parameter space:

```{r, eval=FALSE}
fit_noninformative <- stan(paste0(path, "stan_models/wt_disp_noninformative.stan"), data=dada, model_name = "noninf", seed = 38)

fit_df$prior <- "Original"
fit_df2 <- as.data.frame(fit_noninformative)[1:4]
fit_df2$prior <- "Noninformative"

fit_df_both <- rbind(fit_df, fit_df2)
fit_df_both$prior <- factor(fit_df_both$prior, levels = c("Original", "Noninformative"))

p_params2 <- ggplot(fit_df_both, aes(beta_wt, beta_disp, color=sigma)) +
  geom_point(alpha = 0.3) +
  geom_density2d(color = "white") +
  scale_color_viridis_c(option = 'inferno') +
  labs(title = 'Stan parameter space with different priors') +
  theme_bw() +
  facet_grid(cols=vars(prior))
p_params2
```

Code producing plots of marginal distributions:

```{r, eval=FALSE}
wt_p2 <- ggplot(fit_df_both, aes(beta_wt, color=prior)) +
  geom_density() +
  theme_bw()

disp_p2 <- ggplot(fit_df_both, aes(beta_disp, color = prior)) +
  geom_density() +
  theme_bw()

grid.arrange(wt_p2, disp_p2, ncol = 2)
```

