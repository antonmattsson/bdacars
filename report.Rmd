---
title: "bdacars"
output:
  html_document:
    df_print: paged
---

## Introduction

If you're reading this, you have probably heard about mtcars. If not, you're about to.

Originally extracted from the US magazine Motor Trend in the 1970s, this dataset of car performance and design information has inspired a load of applications of statistical models, notably in regression. For some reason, we have never actually analyzed this dataset on any course before, and now is our chance to take a swing at it. The Bayesian way. Let's see what we have on our hands:

### The dataset

The dataset consists of 32 cars, for which the following information has been recorded:

- Miles per gallon  (a measure of fuel consumption)
- Number of cylinders
- Displacement (the total volume of the cylinders)
- Gross horsepower
- Rear axle ratio (related to towing capabilities)
- Weight
- ¼ mile time (how fast the car can traverse a quarter mile)
- shape of the engine – straight vs V-shaped
- Transmission  - automatic vs manual
- Number of forward gears
- Number of carburetors

The dataset naturally calls for a regression model to study the effect of car design choices on fuel consumption. Here's our approach:

### The analysis problem

We will analyze three different factors potentially affecting fuel consumption: car weight, displacement and transmission. We hypothesize that heavier cars with bigger engines should consume more fuel i.e. they should have a lower miles per gallon (mpg) score. Altough we do assume that weight and displacement are correlated, so it'll be interesting to see how we can separate the two effects.  We also hypothesize that automatic cars consume less fuel than manual cars, since the automatic gearbox can optimize gears used better than a human driver.

To find out which of the three variables best predict fuel consumption, we will try four different models: one model for each pair of predictors and a model with all three included. We will then choose the best model and continue with the analysis.

### Unit conversions

The dataset is quite clean: no missing data or any nasty things to deal with. There is one problem we need to tackle right away, though. The units. Miles per gallon is not very easy to interpret to a Finnish person, especially because we are used to measuring fuel consumption directly, instead of measuring how far we can get with a fixed volume of fuel. Also, car displacement is recorded in cubic inches and car weight in pounds. Yep, the first step is to convert them to Finnish units so we don't need to do mental work interpreting every single figure and coefficient. Here are the conversions:

- Fuel consumption: Miles per US gallon $\rightarrow$ litres per 100 km
- Weight: 1000 pounds $\rightarrow$ metric tons (1000 kg)
- Displacement: Cubic inches $\rightarrow$ litres

Before deciding on further preprocessing steps, we need to emplore the data a bit.

## Exploratory analysis

Setting up libraries and the project path:

```{r, message=FALSE}
library(rstan)
library(GGally)
library(dplyr)
library(loo)
library(gridExtra)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
path <- "~/Google_Drive/Kurssit/BDA/project/bdacars/"
source(paste0(path, "functions.R"))
```

Apply the unit conversions:

```{r}
propercars <- data.frame(wt = mtcars$wt/2.205,
                         disp = mtcars$disp/61.024,
                         lp100km = 235.215/mtcars$mpg,
                         am = mtcars$am)
```

Now that we got the units right, let's take a lok at the data! A good first step is to plot the variables against each other. The ```ggpairs``` function fomr the ```GGally``` package comes in handy:

```{r, fig.width=10, fig.height=10}
# Change am to factor for correct plotting
plotcars <- propercars
plotcars$am <- factor(propercars$am)
# Change default plots styled for the lower triangular of the plot matrix
lower <- list(continuous = "smooth",
                combo = "facetdensity")
# Plot pairs of variables against each other
ggpairs(plotcars,
        lower = lower,
        showStrips = TRUE,
        progress = FALSE) +
  theme_bw()
```

The plot matrix is a lot to take in, so let's deal with it piece-by-piece.  
First, let's look at the third row. It displays fuel consumption as a function of the potential explanatory variables. Starting from the left, the first three plots show no surprises. Fuel consumption strongly correlates with car weight and displacement, and the distribution looks smooth with few cars deviating from the rest. The last plot, though, seems to contradict one our initial hypotheses, since manual cars (class 1) seem to have lower fuel consumption than automatic ones. The bottom row displays a closer view at the distribution of the consumption in the two gearbox classes. The distribution of automatic cars seems to follow the overall distribution, while cars with low fuel consumption dominate the manual class.  We'll have to come back to this later!

Next up, the top two rows where we observe the relationships between our potential explanatory variables. The short story is : they're correlated. The Pearson correlation coefficient between weight and displacement is just shy of 0.9. In addition, automatic cars seem to be heavier an equipped with larger engines than manual cars. Maybe this explains their higher fuel consumption! We'll see after we run our models.

All the relationships in the data seem to be linear, so there is no clear need for transformation of variables. We can now move to the modeling part.

## Modeling

ADD DESCRIPTION OF THE MODELS

Setting up the data into a Stan-friendly format:

```{r}
dada <- list(lp100km = propercars$lp100km,
             wt=propercars$wt,
             disp = propercars$disp,
             am = propercars$am,
             n = nrow(propercars))
```

Next we will run the Stan models:

```{r, echo = T, results = 'hide'}
model_names <- c("wt_disp", "wt_am", "disp_am", "wt_disp_am")
models <- list()

for(mname in model_names){
  fit <- stan(paste0(path, "stan_models/", mname,".stan"), data=dada, model_name = mname)
  models[[mname]] <- fit
}
```

Before comparing the performance of the models, we need to ensure a fair contest by checking that all of them have converged. Hear are the basic convergence diagnostics:

```{r, results='asis'}
for(mname in model_names){
  # Extract convergence diagnostics from the model
  diagnostics <- convergence_diagnostics(models[[mname]], mname)
  # Output as a nice table
  print(knitr::kable(diagnostics, caption = paste("lp100km ~", gsub("_", " + ", mname))))
}
```

Since all the $\hat{R}$ values are close to 1, we can conclude that all the models have converged. We can now compare the models to find the best model for further analysis.

### Model comparison:

We will use leave-one-out cross validation (LOO-cv) to compare the performance of the models:

```{r}
# Initialize
psisloos <- c()
p_effs <- c()
k_plots <- list()
for(mname in model_names){
  # Run LOO-cv
  loglik <- extract_log_lik(models[[mname]], merge_chains = FALSE)
  r_eff <- relative_eff(exp(loglik))
  loocv <- suppressWarnings(loo(loglik, r_eff = r_eff))
  
  # PSIS-LOO value
  psisloo <- loocv$estimates[1]
  # Effective parameters
  p_eff <- loocv$estimates[2]
  # k-values
  kval <- loocv$diagnostics$pareto_k
  
  # Combine results from the models
  p_effs <- c(p_effs, p_eff)
  psisloos <- c(psisloos, psisloo)
  
  # Plot of k-values
  kvals <- data.frame(k = kval,
                      x = 1:length(kval))
  p <- ggplot(data=kvals, aes(x=x, y=k)) +
    geom_point(color = 'red') +
    geom_hline(yintercept=0.5, color = 'blue') +
    ggtitle(mname) +
    xlab("") +
    ylab("k-value") +
    theme_bw()
  k_plots <- c(k_plots, list(p))
}
```

The first step is to see if the k-values of LOO-cv are low enough so that we can trust the results:

```{r, fig.width=10, fig.height=10}
# Combine plots of the k-values
grid.arrange(k_plots[[1]], k_plots[[2]], k_plots[[3]], k_plots[[4]], nrow=4, ncol=1)
```

With a few exceptions, most of the k-values seem to be low enough so that we can trust the results. The only one of the models that is raising some concern is the one with weight and displacement as the explanatory variables.

Next, enter drumroll, let's look at the PSIS-LOO values. The higher the value, the better the performance of the model. Winner takes it all, other models are discarded before we continue any further.


```{r}
psisloosdf <- data.frame(psisloo = psisloos,
                          model = model_names)
ggplot(data=psisloosdf, aes(x=model, y=psisloo)) + geom_bar(stat="identity", width=0.5) +
   geom_hline(yintercept=max(psisloos), color = 'blue')
```

```{r}
psisloosdf %>%
  arrange(desc(psisloo))
```

THE TRIFORCE WINS!!!!